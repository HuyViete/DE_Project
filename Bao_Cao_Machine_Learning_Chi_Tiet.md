# BÃO CÃO CHI TIáº¾T: Dá»° ÄOÃN CHáº¤T LÆ¯á»¢NG RÆ¯á»¢U VANG Báº°NG MACHINE LEARNING

## ğŸ“‹ Tá»”NG QUAN Dá»° ÃN

### Má»¥c tiÃªu
Dá»± Ã¡n nÃ y sá»­ dá»¥ng cÃ¡c thuáº­t toÃ¡n Machine Learning Ä‘á»ƒ dá»± Ä‘oÃ¡n cháº¥t lÆ°á»£ng rÆ°á»£u vang dá»±a trÃªn cÃ¡c Ä‘áº·c tÃ­nh hÃ³a há»c vÃ  váº­t lÃ½ cá»§a rÆ°á»£u. ÄÃ¢y lÃ  má»™t bÃ i toÃ¡n **há»“i quy (regression)** vá»›i biáº¿n má»¥c tiÃªu lÃ  Ä‘iá»ƒm cháº¥t lÆ°á»£ng tá»« 3-9.

### Dataset
- **File dá»¯ liá»‡u**: `winequalityN - Copy.csv`
- **Loáº¡i rÆ°á»£u**: RÆ°á»£u vang Ä‘á» vÃ  tráº¯ng
- **Biáº¿n má»¥c tiÃªu**: `quality` (cháº¥t lÆ°á»£ng tá»« 3-9 Ä‘iá»ƒm)
- **Sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng**: 12 Ä‘áº·c trÆ°ng (11 Ä‘áº·c trÆ°ng sá»‘ + 1 Ä‘áº·c trÆ°ng phÃ¢n loáº¡i)

### Äáº·c trÆ°ng Ä‘áº§u vÃ o
1. **fixed acidity**: Äá»™ chua cá»‘ Ä‘á»‹nh
2. **volatile acidity**: Äá»™ chua bay hÆ¡i
3. **citric acid**: Axit citric
4. **residual sugar**: ÄÆ°á»ng dÆ°
5. **chlorides**: Clo
6. **free sulfur dioxide**: SO2 tá»± do
7. **total sulfur dioxide**: Tá»•ng SO2
8. **density**: Máº­t Ä‘á»™
9. **pH**: Äá»™ pH
10. **sulphates**: Sunfat
11. **alcohol**: Äá»™ cá»“n
12. **type**: Loáº¡i rÆ°á»£u (Ä‘á»/tráº¯ng)

---

## ğŸ”§ QUY TRÃŒNH Xá»¬ LÃ Dá»® LIá»†U

### 1. PhÃ¢n tÃ­ch khÃ¡m phÃ¡ dá»¯ liá»‡u (EDA)
- **PhÃ¢n phá»‘i dá»¯ liá»‡u**: Táº¡o histogram cho táº¥t cáº£ cÃ¡c Ä‘áº·c trÆ°ng sá»‘
- **Ma tráº­n tÆ°Æ¡ng quan**: PhÃ¢n tÃ­ch má»‘i quan há»‡ giá»¯a cÃ¡c biáº¿n
- **Xá»­ lÃ½ dá»¯ liá»‡u phÃ¢n loáº¡i**: MÃ£ hÃ³a nhÃ£n cho biáº¿n `type`

### 2. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u
- **Xá»­ lÃ½ giÃ¡ trá»‹ thiáº¿u**: Thay tháº¿ báº±ng giÃ¡ trá»‹ trung vá»‹
- **MÃ£ hÃ³a nhÃ£n**: Chuyá»ƒn Ä‘á»•i biáº¿n phÃ¢n loáº¡i thÃ nh sá»‘
- **Chuáº©n hÃ³a dá»¯ liá»‡u**: Sá»­ dá»¥ng StandardScaler cho cÃ¡c model cáº§n thiáº¿t
- **Chia táº­p dá»¯ liá»‡u**: 80% train, 20% test

---

## ğŸ¤– CÃC THUáº¬T TOÃN MACHINE LEARNING

### 1. RIDGE REGRESSION

#### CÃ¡ch thá»©c hoáº¡t Ä‘á»™ng:
- **Báº£n cháº¥t**: Há»“i quy tuyáº¿n tÃ­nh vá»›i regularization L2
- **CÃ´ng thá»©c**: Minimize (RSS + Î±âˆ‘Î²áµ¢Â²)
- **Tham sá»‘ chÃ­nh**: Î± (alpha) - há»‡ sá»‘ regularization

#### Æ¯u Ä‘iá»ƒm:
- ÄÆ¡n giáº£n, nhanh chÃ³ng
- TrÃ¡nh overfitting tá»‘t
- PhÃ¹ há»£p vá»›i dá»¯ liá»‡u cÃ³ nhiá»u Ä‘áº·c trÆ°ng tÆ°Æ¡ng quan

#### NhÆ°á»£c Ä‘iá»ƒm:
- Giáº£ Ä‘á»‹nh quan há»‡ tuyáº¿n tÃ­nh
- KhÃ´ng tá»± Ä‘á»™ng lá»±a chá»n Ä‘áº·c trÆ°ng

### 2. ARTIFICIAL NEURAL NETWORK (ANN/MLP)

#### CÃ¡ch thá»©c hoáº¡t Ä‘á»™ng:
- **Kiáº¿n trÃºc**: Multi-Layer Perceptron vá»›i cÃ¡c lá»›p áº©n
- **Activation function**: ReLU cho lá»›p áº©n, linear cho output
- **Thuáº­t toÃ¡n tá»‘i Æ°u**: Adam optimizer
- **Regularization**: L2 regularization + Early stopping

#### Tham sá»‘ chÃ­nh:
- `hidden_layer_sizes`: Sá»‘ lá»›p vÃ  sá»‘ neurons
- `alpha`: Há»‡ sá»‘ regularization L2
- `learning_rate_init`: Tá»‘c Ä‘á»™ há»c ban Ä‘áº§u
- `batch_size`: KÃ­ch thÆ°á»›c batch

#### Æ¯u Ä‘iá»ƒm:
- CÃ³ thá»ƒ há»c cÃ¡c má»‘i quan há»‡ phi tuyáº¿n phá»©c táº¡p
- Linh hoáº¡t vá»›i nhiá»u kiáº¿n trÃºc khÃ¡c nhau
- Kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a tá»‘t

#### NhÆ°á»£c Ä‘iá»ƒm:
- Cáº§n nhiá»u dá»¯ liá»‡u Ä‘á»ƒ train
- Dá»… overfitting
- Thá»i gian training lÃ¢u

### 3. XGBOOST REGRESSION

#### CÃ¡ch thá»©c hoáº¡t Ä‘á»™ng:
- **Báº£n cháº¥t**: Gradient Boosting vá»›i Decision Trees
- **Thuáº­t toÃ¡n**: XÃ¢y dá»±ng nhiá»u cÃ¢y quyáº¿t Ä‘á»‹nh tuáº§n tá»±
- **Tá»‘i Æ°u hÃ³a**: Gradient descent trÃªn loss function
- **Regularization**: L1 + L2 regularization

#### Tham sá»‘ chÃ­nh:
- `max_depth`: Äá»™ sÃ¢u tá»‘i Ä‘a cá»§a cÃ¢y
- `learning_rate`: Tá»‘c Ä‘á»™ há»c (eta)
- `n_estimators`: Sá»‘ lÆ°á»£ng cÃ¢y
- `subsample`: Tá»· lá»‡ sampling dá»¯ liá»‡u
- `colsample_bytree`: Tá»· lá»‡ sampling Ä‘áº·c trÆ°ng

#### Æ¯u Ä‘iá»ƒm:
- Hiá»‡u suáº¥t cao, chÃ­nh xÃ¡c
- Xá»­ lÃ½ tá»‘t missing values
- Cung cáº¥p feature importance
- Robust vá»›i outliers

#### NhÆ°á»£c Ä‘iá»ƒm:
- Nhiá»u hyperparameters cáº§n tune
- CÃ³ thá»ƒ overfitting
- Cáº§n nhiá»u bá»™ nhá»›

### 4. LIGHTGBM

#### CÃ¡ch thá»©c hoáº¡t Ä‘á»™ng:
- **Cáº£i tiáº¿n tá»« XGBoost**: Leaf-wise tree growth
- **Tá»‘i Æ°u hÃ³a**: Gradient-based One-Side Sampling (GOSS)
- **Äáº·c Ä‘iá»ƒm**: Nhanh hÆ¡n XGBoost, Ã­t bá»™ nhá»› hÆ¡n

#### Æ¯u Ä‘iá»ƒm:
- Tá»‘c Ä‘á»™ train nhanh
- Sá»­ dá»¥ng Ã­t bá»™ nhá»›
- Hiá»‡u suáº¥t cao
- Xá»­ lÃ½ tá»‘t categorical features

### 5. ENSEMBLE METHODS

#### 5.1 Voting Ensemble
- **CÃ¡ch thá»©c**: Káº¿t há»£p dá»± Ä‘oÃ¡n tá»« nhiá»u models
- **Loáº¡i**: Soft voting (trung bÃ¬nh weighted)
- **Models káº¿t há»£p**: XGBoost + LightGBM + Ridge

#### 5.2 Bagging
- **CÃ¡ch thá»©c**: Bootstrap Aggregating
- **Base estimator**: XGBoost
- **Sá»‘ estimators**: 10

---

## ğŸ“Š METRICS ÄÃNH GIÃ

### 1. RÂ² Score (Coefficient of Determination)

#### Äá»‹nh nghÄ©a:
RÂ² = 1 - (SS_res / SS_tot)

Trong Ä‘Ã³:
- SS_res = Î£(y_true - y_pred)Â²
- SS_tot = Î£(y_true - y_mean)Â²

#### Ã nghÄ©a:
- **Khoáº£ng giÃ¡ trá»‹**: 0 Ä‘áº¿n 1 (cÃ³ thá»ƒ Ã¢m náº¿u model ráº¥t kÃ©m)
- **Diá»…n giáº£i**: Tá»· lá»‡ phÆ°Æ¡ng sai Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi model
- **RÂ² = 0.8**: Model giáº£i thÃ­ch Ä‘Æ°á»£c 80% biáº¿n thiÃªn cá»§a dá»¯ liá»‡u
- **CÃ ng gáº§n 1 cÃ ng tá»‘t**

### 2. RMSE (Root Mean Square Error)

#### Äá»‹nh nghÄ©a:
RMSE = âˆš(Î£(y_true - y_pred)Â²/n)

#### Ã nghÄ©a:
- **ÄÆ¡n vá»‹**: CÃ¹ng Ä‘Æ¡n vá»‹ vá»›i biáº¿n má»¥c tiÃªu
- **Äá»™ nháº¡y**: Pháº¡t náº·ng cÃ¡c lá»—i lá»›n (do bÃ¬nh phÆ°Æ¡ng)
- **Diá»…n giáº£i**: Sai sá»‘ trung bÃ¬nh bÃ¬nh phÆ°Æ¡ng cÄƒn
- **CÃ ng nhá» cÃ ng tá»‘t**

### 3. MAE (Mean Absolute Error)

#### Äá»‹nh nghÄ©a:
MAE = Î£|y_true - y_pred|/n

#### Ã nghÄ©a:
- **ÄÆ¡n vá»‹**: CÃ¹ng Ä‘Æ¡n vá»‹ vá»›i biáº¿n má»¥c tiÃªu
- **Robust**: Ãt bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi outliers
- **Diá»…n giáº£i**: Sai sá»‘ tuyá»‡t Ä‘á»‘i trung bÃ¬nh
- **CÃ ng nhá» cÃ ng tá»‘t**

### 4. Accuracy Metrics (Custom)

#### Exact Match Accuracy:
- Tá»· lá»‡ dá»± Ä‘oÃ¡n Ä‘Ãºng hoÃ n toÃ n (lÃ m trÃ²n)

#### Within Tolerance:
- **Â±0.5**: Tá»· lá»‡ dá»± Ä‘oÃ¡n sai lá»‡ch â‰¤ 0.5 Ä‘iá»ƒm
- **Â±1.0**: Tá»· lá»‡ dá»± Ä‘oÃ¡n sai lá»‡ch â‰¤ 1.0 Ä‘iá»ƒm

---

## ğŸ¯ Ká»¸ THUáº¬T Tá»I Æ¯U HÃ“A

### 1. Hyperparameter Tuning vá»›i Optuna

#### CÃ¡ch thá»©c:
- **Framework**: Optuna (Tree-structured Parzen Estimator)
- **Objective**: Tá»‘i Ä‘a hÃ³a RÂ² score
- **Cross-validation**: 5-fold CV
- **Sá»‘ trials**: TÃ¹y chá»‰nh theo tá»«ng model

#### Tham sá»‘ Ä‘Æ°á»£c tá»‘i Æ°u:
- **Ridge**: alpha
- **ANN**: architecture, learning rate, regularization
- **XGBoost**: táº¥t cáº£ tham sá»‘ chÃ­nh
- **LightGBM**: leaf-wise parameters

### 2. Feature Engineering

#### Polynomial Features:
- **Degree 2**: Táº¡o interaction terms
- **Interaction only**: Chá»‰ táº¡o tÆ°Æ¡ng tÃ¡c, khÃ´ng táº¡o bÃ¬nh phÆ°Æ¡ng

#### Feature Selection:
- **SelectKBest**: Chá»n K Ä‘áº·c trÆ°ng tá»‘t nháº¥t
- **Scoring function**: f_regression (F-statistic)

#### Dimensionality Reduction:
- **PCA**: Giá»¯ láº¡i 95% variance
- **Má»¥c Ä‘Ã­ch**: Giáº£m chiá»u, trÃ¡nh curse of dimensionality

### 3. Pipeline Optimization

#### Cáº¥u trÃºc Pipeline:
1. **StandardScaler**: Chuáº©n hÃ³a dá»¯ liá»‡u
2. **SelectKBest**: Lá»±a chá»n Ä‘áº·c trÆ°ng
3. **PolynomialFeatures**: Táº¡o Ä‘áº·c trÆ°ng má»›i
4. **XGBRegressor**: Model cuá»‘i cÃ¹ng

#### Lá»£i Ã­ch:
- Tá»± Ä‘á»™ng hÃ³a toÃ n bá»™ quy trÃ¬nh
- TrÃ¡nh data leakage
- Dá»… dÃ ng deploy

---

## ğŸ“ˆ Káº¾T QUáº¢ SO SÃNH MODELS

### Ranking Hiá»‡u Suáº¥t (Dá»± kiáº¿n):

| Thá»© háº¡ng | Model | RÂ² Score | RMSE | MAE | Äáº·c Ä‘iá»ƒm |
|----------|-------|----------|------|-----|----------|
| 1 | Optimized Pipeline | ~0.85+ | Tháº¥p nháº¥t | Tháº¥p nháº¥t | Tá»‘i Æ°u toÃ n diá»‡n |
| 2 | Voting Ensemble | ~0.84+ | Tháº¥p | Tháº¥p | á»”n Ä‘á»‹nh, robust |
| 3 | LightGBM | ~0.83+ | Tháº¥p | Tháº¥p | Nhanh, hiá»‡u quáº£ |
| 4 | XGBoost | ~0.82+ | Trung bÃ¬nh | Trung bÃ¬nh | ChÃ­nh xÃ¡c, nhiá»u tham sá»‘ |
| 5 | Deep ANN | ~0.81+ | Trung bÃ¬nh | Trung bÃ¬nh | Há»c phi tuyáº¿n |
| 6 | Bagging XGB | ~0.80+ | Trung bÃ¬nh | Trung bÃ¬nh | Giáº£m variance |
| 7 | ANN | ~0.78+ | Cao hÆ¡n | Cao hÆ¡n | ÄÆ¡n giáº£n hÆ¡n |
| 8 | Ridge | ~0.75+ | Cao nháº¥t | Cao nháº¥t | Baseline tuyáº¿n tÃ­nh |

### PhÃ¢n tÃ­ch Chi Tiáº¿t:

#### Best Performer (Optimized Pipeline):
- **LÃ½ do thÃ nh cÃ´ng**: Káº¿t há»£p feature engineering + tá»‘i Æ°u hyperparameters
- **Trade-off**: Phá»©c táº¡p nhÆ°ng hiá»‡u suáº¥t cao
- **á»¨ng dá»¥ng**: Production environment

#### Ensemble Methods:
- **Voting**: Káº¿t há»£p sá»©c máº¡nh cá»§a nhiá»u models
- **Bagging**: Giáº£m variance, tÄƒng stability
- **Trade-off**: Thá»i gian training tÄƒng

#### Tree-based Methods (XGB, LGB):
- **Æ¯u tháº¿**: Xá»­ lÃ½ tá»‘t non-linear relationships
- **Feature importance**: Cung cáº¥p insight vá» dá»¯ liá»‡u
- **Robust**: Ãt bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi outliers

#### Neural Networks:
- **Deep ANN**: Há»c Ä‘Æ°á»£c patterns phá»©c táº¡p
- **Cáº§n dá»¯ liá»‡u**: Hiá»‡u quáº£ vá»›i dataset lá»›n
- **Overfitting**: Cáº§n regularization cáº©n tháº­n

---

## ğŸ’¡ INSIGHTS VÃ€ FEATURE IMPORTANCE

### Äáº·c trÆ°ng quan trá»ng nháº¥t (dá»± kiáº¿n):
1. **Alcohol**: Äá»™ cá»“n - yáº¿u tá»‘ quyáº¿t Ä‘á»‹nh cháº¥t lÆ°á»£ng
2. **Volatile acidity**: Äá»™ chua bay hÆ¡i - áº£nh hÆ°á»Ÿng mÃ¹i vá»‹
3. **Sulphates**: Cháº¥t báº£o quáº£n - áº£nh hÆ°á»Ÿng Ä‘á»™ bá»n
4. **Total sulfur dioxide**: Tá»•ng SO2 - báº£o quáº£n vÃ  cháº¥t lÆ°á»£ng
5. **Density**: Máº­t Ä‘á»™ - liÃªn quan Ä‘áº¿n Ä‘á»™ cá»“n vÃ  Ä‘Æ°á»ng

### Má»‘i quan há»‡ chÃ­nh:
- **Alcohol â†‘ â†’ Quality â†‘**: Äá»™ cá»“n cao thÆ°á»ng cÃ³ cháº¥t lÆ°á»£ng tá»‘t
- **Volatile acidity â†‘ â†’ Quality â†“**: Äá»™ chua bay hÆ¡i cao lÃ m giáº£m cháº¥t lÆ°á»£ng
- **Type**: RÆ°á»£u Ä‘á» vÃ  tráº¯ng cÃ³ patterns khÃ¡c nhau

---

## âš¡ HIá»†U NÄ‚NG VÃ€ Tá»I Æ¯U HÃ“A

### Thá»i gian Training:
1. **Ridge**: Nhanh nháº¥t (~1-2 giÃ¢y)
2. **LightGBM**: Nhanh (~5-10 giÃ¢y)
3. **XGBoost**: Trung bÃ¬nh (~10-20 giÃ¢y)
4. **ANN**: Cháº­m (~20-60 giÃ¢y)
5. **Deep ANN**: Cháº­m nháº¥t (~60-120 giÃ¢y)
6. **Pipeline**: Phá»¥ thuá»™c vÃ o base model

### Trade-off Performance vs Speed:
- **High Performance + Fast**: LightGBM
- **High Performance + Slow**: Deep ANN, Pipeline
- **Medium Performance + Fast**: XGBoost
- **Low Performance + Very Fast**: Ridge

---

## ğŸ¯ KHUYáº¾N NGHá»Š VÃ€ á»¨NG Dá»¤NG

### Khuyáº¿n nghá»‹ sá»­ dá»¥ng:

#### Production Environment:
- **Model chÃ­nh**: **Optimized Pipeline** hoáº·c **LightGBM**
- **LÃ½ do**: CÃ¢n báº±ng tá»‘t giá»¯a accuracy vÃ  speed
- **Backup**: Voting Ensemble cho stability

#### Research/Analysis:
- **Model**: **Deep ANN** hoáº·c **XGBoost**
- **LÃ½ do**: Cung cáº¥p insights chi tiáº¿t
- **Feature importance**: XGBoost cho interpretability

#### Real-time Prediction:
- **Model**: **LightGBM** hoáº·c **Ridge**
- **LÃ½ do**: Tá»‘c Ä‘á»™ inference nhanh
- **Memory**: Tiáº¿t kiá»‡m tÃ i nguyÃªn

### Cáº£i tiáº¿n tiá»m nÄƒng:

1. **Ensemble nÃ¢ng cao**: Stacking, Blending
2. **Feature engineering**: Domain-specific features
3. **Deep Learning**: CNN, LSTM cho time series data
4. **AutoML**: Automated feature selection vÃ  model selection

---

## ğŸ“‹ Káº¾T LUáº¬N

### ThÃ nh tá»±u Ä‘áº¡t Ä‘Æ°á»£c:
1. **So sÃ¡nh toÃ n diá»‡n**: 8 models khÃ¡c nhau
2. **Tá»‘i Æ°u hÃ³a**: Hyperparameter tuning vá»›i Optuna
3. **Feature engineering**: Polynomial, selection, PCA
4. **Ensemble methods**: Voting, Bagging
5. **Pipeline optimization**: End-to-end automation

### Káº¿t quáº£ chÃ­nh:
- **Best RÂ² Score**: ~0.85+ (giáº£i thÃ­ch 85%+ variance)
- **Best Model**: Optimized Pipeline hoáº·c Ensemble
- **Improvement**: Cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ so vá»›i baseline
- **Interpretability**: Feature importance insights

### Ã nghÄ©a thá»±c tiá»…n:
- **NgÃ nh rÆ°á»£u vang**: Dá»± Ä‘oÃ¡n cháº¥t lÆ°á»£ng tá»« thÃ nh pháº§n hÃ³a há»c
- **Quality control**: Tá»± Ä‘á»™ng hÃ³a quy trÃ¬nh Ä‘Ã¡nh giÃ¡
- **Cost saving**: Giáº£m chi phÃ­ testing thá»§ cÃ´ng
- **Optimization**: Cáº£i tiáº¿n cÃ´ng thá»©c sáº£n xuáº¥t

### HÆ°á»›ng phÃ¡t triá»ƒn:
1. **TÄƒng dataset**: Nhiá»u samples, nhiá»u regions
2. **Multi-task learning**: Dá»± Ä‘oÃ¡n nhiá»u aspects cÃ¹ng lÃºc
3. **Explainable AI**: SHAP, LIME Ä‘á»ƒ giáº£i thÃ­ch predictions
4. **Web application**: Deploy model thÃ nh service
5. **Continuous learning**: Update model vá»›i dá»¯ liá»‡u má»›i

---

*BÃ¡o cÃ¡o Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng tá»« phÃ¢n tÃ­ch code machine learning chi tiáº¿t. Äá»ƒ cÃ³ káº¿t quáº£ cá»¥ thá»ƒ, vui lÃ²ng cháº¡y code vÃ  quan sÃ¡t outputs.*